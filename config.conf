base = {
  # model
  model: NeuralNetwork
  batch_size: 256
  epochs: 20

  # learning rate
  learning_rate: 0.001
  learning_rate_decay {
    enabled: true
    decay_steps: 100
    decay_rate: 0.999
    staircase: false
  }
  weight_decay: 0.0001

  # optimizer
  optimizer: adam

  # ours params
  checkpoint_every: 10
}

tmp = ${base}{
}
1.ID1_NeuralNetwork_adam_lr_0.001 = ${base}{
}

2.ID1_NeuralNetwork_adam_lr_0.01 = ${base}{
    learning_rate: 0.01
}

3.ID1_NeuralNetwork_adam_lr_0.0001 = ${base}{
    learning_rate: 0.0001
}

4.ID1_NeuralNetwork_adam_lr_0.0005 = ${base}{
    learning_rate: 0.0005
}

5.ID1_NeuralNetwork_adam_lr_0.00001 = ${base}{
    learning_rate: 0.00001
    epochs: 40
}

6.ID1_NeuralNetwork_adam_lr_0.001 = ${base}{
    model: BinaryClassification
}

7.ID1_NeuralNetwork_adam_lr_0.01 = ${6.ID1_NeuralNetwork_adam_lr_0.001}{
    learning_rate: 0.01
}

8.ID1_NeuralNetwork_adam_lr_0.0001 = ${6.ID1_NeuralNetwork_adam_lr_0.001}{
    learning_rate: 0.0001
}

9.ID1_NeuralNetwork_adam_lr_0.0005 = ${6.ID1_NeuralNetwork_adam_lr_0.001}{
    learning_rate: 0.0005
}

10.ID1_NeuralNetwork_adam_lr_0.00001 = ${6.ID1_NeuralNetwork_adam_lr_0.001}{
    learning_rate: 0.00001
    epochs: 40
}



11.ID1_NeuralNetwork_adam_lr_0.001 = ${base}{
    model: BinaryClassificationOneLayer
}

12.ID1_NeuralNetwork_adam_lr_0.01 = ${11.ID1_NeuralNetwork_adam_lr_0.001}{
    learning_rate: 0.01
}

13.ID1_NeuralNetwork_adam_lr_0.0001 = ${11.ID1_NeuralNetwork_adam_lr_0.001}{
    learning_rate: 0.0001
}

14.ID1_NeuralNetwork_adam_lr_0.0005 = ${11.ID1_NeuralNetwork_adam_lr_0.001}{
    learning_rate: 0.0005
}

15.ID1_NeuralNetwork_adam_lr_0.00001 = ${11.ID1_NeuralNetwork_adam_lr_0.001}{
    learning_rate: 0.00001
    epochs: 40
}


