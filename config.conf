base = {
  # model
  model: NeuralNetwork
  batch_size: 256
  epochs: 20

  # learning rate
  learning_rate: 0.001
  learning_rate_decay {
    enabled: true
    decay_steps: 100
    decay_rate: 0.999
    staircase: false
  }
  weight_decay: 0.0001

  # optimizer
  optimizer: adam

  # ours params
  checkpoint_every: 10

  loss: CrossEntropyLoss
}

tmp = ${base}{
}

###############################################################################
# learning rate experiments
1.ID1_NeuralNetwork_adam_lr_0.001 = ${base}{
}

1.ID1_BigNeuralNetwork_adam_lr_0.001 = ${base}{
    model: BigNeuralNetwork
}

2.ID1_NeuralNetwork_adam_lr_0.01 = ${base}{
    learning_rate: 0.01
}

3.ID1_NeuralNetwork_adam_lr_0.0001 = ${base}{
    learning_rate: 0.0001
    epochs: 80
}

4.ID1_NeuralNetwork_adam_lr_0.0005 = ${base}{
    learning_rate: 0.0005
}

5.ID1_NeuralNetwork_adam_lr_0.00001 = ${base}{
    learning_rate: 0.00001
    epochs: 120
}

6.ID1_NeuralNetwork_adam_lr_0.1 = ${base}{
    learning_rate: 0.1
    epochs: 20
}

7.ID1_NeuralNetwork_adam_lr_5 = ${base}{
    learning_rate: 5
    epochs: 20
}
###############################################################################




# Big model vs small model
###############################################################################
1.ID1_NeuralNetwork_adam_lr_0.001 = ${base}{
}

1.ID1_BigNeuralNetwork_adam_lr_0.001 = ${base}{
    model: BigNeuralNetwork
}
###############################################################################




# Architecture number 2
###############################################################################

6.ID1_NeuralNetwork_adam_lr_0.001 = ${base}{
    model: BinaryClassification
}

7.ID1_NeuralNetwork_adam_lr_0.01 = ${6.ID1_NeuralNetwork_adam_lr_0.001}{
    learning_rate: 0.01
}

8.ID1_NeuralNetwork_adam_lr_0.0001 = ${6.ID1_NeuralNetwork_adam_lr_0.001}{
    learning_rate: 0.0001
}

9.ID1_NeuralNetwork_adam_lr_0.0005 = ${6.ID1_NeuralNetwork_adam_lr_0.001}{
    learning_rate: 0.0005
}

10.ID1_NeuralNetwork_adam_lr_0.00001 = ${6.ID1_NeuralNetwork_adam_lr_0.001}{
    learning_rate: 0.00001
    epochs: 40
}
###############################################################################




# Architecture number 3
###############################################################################
11.ID1_NeuralNetwork_adam_lr_0.001 = ${base}{
    model: BinaryClassificationOneLayer
    epochs: 30
}

12.ID1_NeuralNetwork_adam_lr_0.01 = ${11.ID1_NeuralNetwork_adam_lr_0.001}{
    learning_rate: 0.01
}

13.ID1_NeuralNetwork_adam_lr_0.0001 = ${11.ID1_NeuralNetwork_adam_lr_0.001}{
    learning_rate: 0.0001
}

#################### Best model! ####################
14.ID1_NeuralNetwork_adam_lr_0.0005 = ${11.ID1_NeuralNetwork_adam_lr_0.001}{
    learning_rate: 0.0005
}
#####################################################

15.ID1_NeuralNetwork_adam_lr_0.00001 = ${11.ID1_NeuralNetwork_adam_lr_0.001}{
    learning_rate: 0.00001
    epochs: 40
}
###############################################################################




###############################################################################
# batch size experiments
16.ID1_NeuralNetwork_adam_batch_16_lr_0.0001 = ${base}{
    learning_rate: 0.0001
    batch_size: 16
}

17.ID1_NeuralNetwork_adam_batch_64_lr_0.0001 = ${base}{
    learning_rate: 2e-4
    batch_size: 64
}

18.ID1_NeuralNetwork_adam_batch_256_lr_0.0001 = ${base}{
    learning_rate: 1e-5
    batch_size: 256
}

19.ID1_NeuralNetwork_adam_batch_512_lr_0.0001 = ${base}{
    batch_size: 512
}

20.ID1_NeuralNetwork_adam_batch_1024_lr_0.0001 = ${base}{
    learning_rate: 1e-3
    batch_size: 1024
}
###############################################################################



# Adam vs Adamw vs SGD
###############################################################################
21.ID1_NeuralNetwork_adam_lr_0.001 = ${base}{
  optimizer: adam
}

22.ID1_NeuralNetwork_adamw_lr_0.001 = ${base}{
  optimizer: adamw
}

23.ID1_NeuralNetwork_SGD_lr_0.001 = ${base}{
  optimizer: SGD
}
###############################################################################



# CrossEntropyLoss vs MSELoss vs SGD
###############################################################################
24.ID1_NeuralNetwork_adam_lr_0.001_CrossEntropyLoss = ${4.ID1_NeuralNetwork_adam_lr_0.0005}{
}

25.ID1_NeuralNetwork_adam_lr_0.001_MSELoss = ${4.ID1_NeuralNetwork_adam_lr_0.0005}{
  loss: MSELoss
}
###############################################################################
