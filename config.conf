base = {
  # model
  model: NeuralNetwork
  batch_size: 256
  epochs: 20

  # learning rate
  learning_rate: 0.001
  learning_rate_decay {
    enabled: true
    decay_steps: 100
    decay_rate: 0.999
    staircase: false
  }
  weight_decay: 0.0001

  # optimizer
  optimizer: adam

  # ours params
  checkpoint_every: 10
}

1.ID1_NeuralNetwork_adam_lr_0.001 = ${base}{
}

2.ID1_NeuralNetwork_adam_lr_0.01 = ${base}{
    learning_rate: 0.01
}

3.ID1_NeuralNetwork_adam_lr_0.0001 = ${base}{
    learning_rate: 0.0001
}

4.ID1_NeuralNetwork_adam_batch_128_lr_0.0001 = ${base}{
    learning_rate: 0.0001
    batch_size: 128
}

5.ID1_NeuralNetwork_adam_batch_128_lr_2e-4 = ${base}{
    learning_rate: 2e-4
    batch_size: 128
}

6.ID1_NeuralNetwork_adam_batch_128_lr_1e-5 = ${base}{
    learning_rate: 1e-5
    batch_size: 128
}

7.ID1_NeuralNetwork_adam_batch_64 = ${base}{
    batch_size: 64
}

8.ID1_NeuralNetwork_adam_batch_128_lr_1e-3 = ${base}{
    learning_rate: 1e-3
    batch_size: 128
}

9.ID1_NeuralNetwork_adam_batch_128_lr_0.0001_epoch_40 = ${base}{
    learning_rate: 0.0001
    batch_size: 128
    epochs: 40
}

10.ID1_NeuralNetwork_adam_batch_128_lr_0.001 = ${base}{
    learning_rate: 0.001
    batch_size: 128
}

11.ID1_NeuralNetwork_adam_batch_64_lr_0.0001 = ${base}{
    learning_rate: 0.0001
    batch_size: 64
}

12.ID1_NeuralNetwork_adam_batch_128_lr_0.001_epoch_40 = ${base}{
    learning_rate: 0.001
    batch_size: 128
    epochs: 40
}

13.ID1_NeuralNetwork_adam_batch_128_lr_0.01_epoch_40 = ${base}{
    learning_rate: 0.01
    batch_size: 128
    epochs: 40
}
